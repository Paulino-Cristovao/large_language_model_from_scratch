{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Implementing Large Language Models\n",
    "\n",
    "In this chapter, we will code attention mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "code_cell_1"
   },
   "outputs": [],
   "source": [
    "# Implement attention mechanism in the decoder \"simple example\"\n",
    "import torch as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1776, -0.2477, -1.4939,  0.5932, -0.4045,  0.8290],\n",
      "        [-0.0125,  0.4582, -2.3477,  0.2177, -0.1264, -0.0633],\n",
      "        [-0.3668,  0.0942,  1.5271,  1.3618, -0.2063,  0.4015],\n",
      "        [-0.2211, -0.5934,  1.7383,  0.5103, -1.6741, -0.0507],\n",
      "        [ 0.7186, -0.3664,  1.6488,  0.9783,  0.8619,  0.7014],\n",
      "        [ 0.9988,  0.6478,  0.9972,  0.2161,  1.3627,  1.6040]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the sentence\n",
    "sentence = \"Your journey starts with one step\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence.split()\n",
    "\n",
    "# Define the size of the embedding (e.g., 10)\n",
    "embedding_size = len(words)\n",
    "\n",
    "# Create a random matrix tensor for the words\n",
    "random_input_matrix = nn.randn(len(words), embedding_size)\n",
    "\n",
    "print(random_input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.5364,  5.7893, -3.2404, -4.0244, -3.9883, -2.2836])\n"
     ]
    }
   ],
   "source": [
    "# Calculate intermediate attention scores \n",
    "\n",
    "# Select the query vector (second word in the sentence)\n",
    "query = random_input_matrix[1]\n",
    "\n",
    "# Initialize an empty tensor to store attention scores\n",
    "attn_scores_2 = nn.empty(random_input_matrix.shape[0])\n",
    "\n",
    "# Calculate attention scores by taking dot product of each word vector with the query vector\n",
    "for i, x_i in enumerate(random_input_matrix):\n",
    "    attn_scores_2[i] = nn.dot(x_i, query)\n",
    "\n",
    "# Print the attention scores\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.5052e-02, 9.0446e-01, 1.0835e-04, 4.9471e-05, 5.1289e-05, 2.8207e-04])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores using softmax function\n",
    "# We use dim=0 to apply softmax across the rows (i.e., across the different word vectors)\n",
    "attn_scores_2 = nn.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "# Print the normalized attention scores\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(nn.sum(attn_scores_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.8826,  3.5364, -0.6487, -1.2517, -2.4055, -1.9199],\n",
      "        [ 3.5364,  5.7893, -3.2404, -4.0244, -3.9883, -2.2836],\n",
      "        [-0.6487, -3.2404,  4.5338,  3.6998,  3.6558,  1.8747],\n",
      "        [-1.2517, -4.0244,  3.6998,  6.4883,  1.9455, -1.1241],\n",
      "        [-2.4055, -3.9883,  3.6558,  1.9455,  5.5610,  4.6355],\n",
      "        [-1.9199, -2.2836,  1.8747, -1.1241,  4.6355,  6.8881]])\n"
     ]
    }
   ],
   "source": [
    "# Conpute the attention score for each word vector\n",
    "attn_scores1 = random_input_matrix @ random_input_matrix.T\n",
    "print(attn_scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.8854e-01, 2.0521e-01, 3.1235e-03, 1.7090e-03, 5.3907e-04, 8.7610e-04],\n",
      "        [9.5052e-02, 9.0446e-01, 1.0835e-04, 4.9471e-05, 5.1289e-05, 2.8207e-04],\n",
      "        [2.9149e-03, 2.1829e-04, 5.1923e-01, 2.2550e-01, 2.1579e-01, 3.6349e-02],\n",
      "        [4.0543e-04, 2.5338e-05, 5.7325e-02, 9.3187e-01, 9.9183e-03, 4.6063e-04],\n",
      "        [2.2060e-04, 4.5312e-05, 9.4624e-02, 1.7108e-02, 6.3595e-01, 2.5205e-01],\n",
      "        [1.3443e-04, 9.3439e-05, 5.9765e-03, 2.9792e-04, 9.4508e-02, 8.9899e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores using softmax function\n",
    "attn_scores2 = nn.softmax(attn_scores1, dim=-1)\n",
    "print(attn_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9314, -0.1017, -1.6503,  0.5183, -0.3467,  0.6436],\n",
      "        [-0.1230,  0.3910, -2.2648,  0.2535, -0.1524,  0.0221],\n",
      "        [-0.0524, -0.1411,  1.5721,  1.0429, -0.2503,  0.4091],\n",
      "        [-0.2200, -0.5510,  1.7236,  0.5637, -1.5628, -0.0162],\n",
      "        [ 0.6700, -0.0710,  1.4737,  0.8143,  0.8433,  0.8876],\n",
      "        [ 0.9634,  0.5482,  1.0616,  0.2951,  1.3047,  1.5107]])\n"
     ]
    }
   ],
   "source": [
    "# compute attention weight for each word vector\n",
    "attn_weights = attn_scores2 @ random_input_matrix\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.w_query = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.w_key = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.w_value = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.w_key\n",
    "        queries = x @ self.w_query\n",
    "        values = x @ self.w_value \n",
    "        attn_scores = queries @ keys.T \n",
    "        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_output = attn_weights @ values\n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3083,  0.6986,  9.0132,  3.4046, -0.5365, -0.1862],\n",
      "        [ 0.4245, -0.5881, -9.7885, -3.0913, -2.4793,  1.7715],\n",
      "        [ 2.6022, -1.7643,  5.5865, -0.0712,  1.6634, -2.3016],\n",
      "        [ 2.6025, -1.7646,  5.5862, -0.0715,  1.6635, -2.3018],\n",
      "        [ 0.8760, -0.2813,  7.6022,  2.1714,  0.1237, -0.9518],\n",
      "        [-3.3342,  2.2808,  7.0788,  4.1153, -0.6294,  2.1286]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "sa_v1 = SelfAttention(embedding_size, embedding_size)\n",
    "attention_score = sa_v1(random_input_matrix)\n",
    "print(attention_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [7.3352e-01, 2.6648e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.8110e-02, 6.1078e-04, 9.5128e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.7980e-02, 6.0881e-04, 9.4810e-01, 3.3095e-03, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1913e-03, 3.7447e-04, 9.9352e-01, 4.3512e-03, 5.6143e-04, 0.0000e+00],\n",
      "        [2.8141e-05, 7.7247e-03, 9.3682e-01, 4.8375e-02, 4.2074e-04, 6.6345e-03]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Hiding future words causal attention \n",
    "# Mask the future words by setting the attention scores to negative infinity\n",
    "attn_scores_masked = torch.softmax(attention_score + torch.triu(torch.full(attention_score.shape, float('-inf')), diagonal=1), dim=-1)\n",
    "print(attn_scores_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# verify that the attention scores are normalized\n",
    "print(attn_scores_masked.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.9026e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.8962e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1229e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.8736e+00, 9.6749e-02, 8.4147e-04, 1.3269e-02]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Make additional attentuion weights with dropout \n",
    "attn_weights_dropout = nn.functional.dropout(attn_scores_masked, p=0.5, training=True)\n",
    "print(attn_weights_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytprch class for casual attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a causal attention mechanism where each token only attends to previous tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert self.embed_dim % self.num_heads == 0, \"Embedding dim must be divisible by num_heads\"\n",
    "\n",
    "        # Query, Key, and Value projection layers\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Mask for causal attention\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(1, 1, 1000, 1000)))  # Assumes max seq length of 1000\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for causal attention.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, seq_len, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying causal self-attention.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        # Project input into Query, Key, and Value\n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply causal mask\n",
    "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        return self.out_proj(attn_output), attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Usage & Testing\n",
    "# Initialize Causal Attention\n",
    "embed_dim = 64   # Embedding size\n",
    "num_heads = 8    # Multi-head attention\n",
    "seq_len = 10     # Sequence length\n",
    "batch_size = 2   # Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "causal_attention = CausalSelfAttention(embed_dim, num_heads)\n",
    "print(causal_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 64])\n",
      "tensor([[[0.1439, 0.5043, 0.1673,  ..., 0.0499, 0.6817, 0.8787],\n",
      "         [0.8150, 0.0649, 0.1663,  ..., 0.1957, 0.2380, 0.6600],\n",
      "         [0.5565, 0.7017, 0.9229,  ..., 0.2101, 0.6002, 0.9715],\n",
      "         ...,\n",
      "         [0.8724, 0.0510, 0.3880,  ..., 0.1878, 0.1430, 0.0822],\n",
      "         [0.5730, 0.2669, 0.5206,  ..., 0.7782, 0.9385, 0.3942],\n",
      "         [0.3370, 0.0276, 0.4640,  ..., 0.9877, 0.4345, 0.0163]],\n",
      "\n",
      "        [[0.2058, 0.4842, 0.7821,  ..., 0.7823, 0.7696, 0.1237],\n",
      "         [0.9298, 0.9635, 0.4529,  ..., 0.4137, 0.4539, 0.3953],\n",
      "         [0.7257, 0.6575, 0.6200,  ..., 0.0690, 0.4895, 0.6925],\n",
      "         ...,\n",
      "         [0.8053, 0.3670, 0.2015,  ..., 0.2986, 0.3613, 0.7340],\n",
      "         [0.3851, 0.4409, 0.9412,  ..., 0.3959, 0.9708, 0.0127],\n",
      "         [0.8741, 0.7567, 0.7489,  ..., 0.3626, 0.3905, 0.9904]]])\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy input\n",
    "x = torch.rand(batch_size, seq_len, embed_dim)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 64])\n",
      "tensor([[[ 0.0388, -0.0762,  0.2145,  ..., -0.1133,  0.2355, -0.1140],\n",
      "         [ 0.0296,  0.0880,  0.2732,  ..., -0.1395,  0.2499, -0.0477],\n",
      "         [-0.0039,  0.1242,  0.2915,  ..., -0.1036,  0.2336, -0.1170],\n",
      "         ...,\n",
      "         [ 0.0970,  0.1154,  0.3041,  ..., -0.0982,  0.2300, -0.0981],\n",
      "         [ 0.0661,  0.0643,  0.2958,  ..., -0.0584,  0.2754, -0.0442],\n",
      "         [ 0.1122,  0.0993,  0.2972,  ..., -0.0535,  0.2828, -0.1044]],\n",
      "\n",
      "        [[ 0.0521,  0.1877,  0.0901,  ..., -0.0352,  0.3724, -0.2702],\n",
      "         [-0.0333,  0.1738,  0.2314,  ..., -0.1156,  0.2846, -0.1275],\n",
      "         [-0.0142,  0.1713,  0.2203,  ..., -0.0401,  0.3044, -0.1009],\n",
      "         ...,\n",
      "         [ 0.0055,  0.0362,  0.2979,  ...,  0.0493,  0.2350, -0.1619],\n",
      "         [ 0.0500,  0.0811,  0.3076,  ...,  0.0121,  0.2659, -0.1488],\n",
      "         [ 0.0149,  0.1058,  0.2782,  ..., -0.0125,  0.2657, -0.1234]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 8, 10, 10])\n",
      "tensor([[[[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5642, 0.5470, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3794, 0.3737, 0.3580,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1453, 0.1381, 0.1400,  ..., 0.1374, 0.0000, 0.0000],\n",
      "          [0.1257, 0.1252, 0.1162,  ..., 0.1288, 0.1118, 0.0000],\n",
      "          [0.1104, 0.1144, 0.1073,  ..., 0.1142, 0.1057, 0.1100]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5484, 0.5628, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3710, 0.3599, 0.3802,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1345, 0.1365, 0.1391,  ..., 0.1452, 0.0000, 0.0000],\n",
      "          [0.1237, 0.1211, 0.1229,  ..., 0.1265, 0.1243, 0.0000],\n",
      "          [0.1026, 0.1112, 0.1145,  ..., 0.1141, 0.1079, 0.1179]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5764, 0.5347, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3654, 0.3566,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1390, 0.1380, 0.0000,  ..., 0.1466, 0.0000, 0.0000],\n",
      "          [0.1263, 0.1175, 0.1183,  ..., 0.1292, 0.1337, 0.0000],\n",
      "          [0.1173, 0.1065, 0.1042,  ..., 0.1154, 0.1141, 0.1102]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5601, 0.5510, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3708, 0.3704, 0.3700,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1478, 0.1335, 0.1384,  ..., 0.1409, 0.0000, 0.0000],\n",
      "          [0.1334, 0.1183, 0.1221,  ..., 0.1224, 0.1213, 0.0000],\n",
      "          [0.1248, 0.0000, 0.1085,  ..., 0.1112, 0.1104, 0.1104]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5422, 0.5689, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3570, 0.3775, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1428, 0.0000, 0.0000,  ..., 0.1367, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1290,  ..., 0.1279, 0.1224, 0.0000],\n",
      "          [0.1090, 0.1106, 0.1135,  ..., 0.1047, 0.1138, 0.0000]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5582, 0.5529, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3598, 0.3762, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1372, 0.1423, 0.1407,  ..., 0.1382, 0.0000, 0.0000],\n",
      "          [0.1257, 0.1270, 0.1239,  ..., 0.1205, 0.1220, 0.0000],\n",
      "          [0.1110, 0.1115, 0.1141,  ..., 0.1078, 0.1156, 0.1093]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5747, 0.5364, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3785, 0.3608, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1430, 0.1367, 0.1459,  ..., 0.1407, 0.0000, 0.0000],\n",
      "          [0.1250, 0.1147, 0.1243,  ..., 0.1291, 0.1243, 0.0000],\n",
      "          [0.1130, 0.1075, 0.1140,  ..., 0.1135, 0.0000, 0.1241]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5738, 0.5373, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4058, 0.3635, 0.3418,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1461, 0.1425, 0.1342,  ..., 0.1468, 0.0000, 0.0000],\n",
      "          [0.1306, 0.1224, 0.1179,  ..., 0.1317, 0.1434, 0.0000],\n",
      "          [0.1199, 0.1079, 0.1057,  ..., 0.1153, 0.1330, 0.1040]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5690, 0.5421, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3809, 0.3609, 0.3692,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1394, 0.1339, 0.1422,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1258, 0.1209, 0.1281,  ..., 0.1269, 0.1242, 0.0000],\n",
      "          [0.1159, 0.1081, 0.0000,  ..., 0.1206, 0.0000, 0.1028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5577, 0.5534, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3853, 0.3742, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1365, 0.1362, 0.1312,  ..., 0.1547, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1217, 0.1199,  ..., 0.1282, 0.1319, 0.0000],\n",
      "          [0.1131, 0.1107, 0.1020,  ..., 0.0000, 0.1207, 0.1029]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5410, 0.5702, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3522, 0.3708, 0.3881,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1320, 0.1389, 0.1436,  ..., 0.1317, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1239, 0.1240,  ..., 0.1157, 0.1218, 0.0000],\n",
      "          [0.1058, 0.1082, 0.1149,  ..., 0.1055, 0.1079, 0.1162]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3768, 0.0000, 0.3572,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1387, 0.1429, 0.1386,  ..., 0.1341, 0.0000, 0.0000],\n",
      "          [0.1223, 0.0000, 0.0000,  ..., 0.1223, 0.1211, 0.0000],\n",
      "          [0.1074, 0.1145, 0.0000,  ..., 0.1070, 0.1053, 0.1214]]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "output, attn_weights = causal_attention(x)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n",
    "print(attn_weights.shape)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-learn-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
